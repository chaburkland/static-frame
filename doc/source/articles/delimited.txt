


Building NumPy Arrays from CSV Files, Faster than Pandas

Building a Faster-than-Pandas CSV-to-NumPy Arrays Reader in C

Going from CSV to NumPy Arrays, Faster than Pandas

After Twenty-Years of CSV Parsing, a Better Conversion to Arrays

Twenty-Years of CSV Parsing

A Faster-than-Pandas CSV Reader with Support for All NumPy Array Types

Building a Faster-than-Pandas CSV Reader with Support for All NumPy Dtypes in C

Extending Python's CSV-Reader C-Extension to Make A High-Performance, Delimited-Text-to-Numpy-Array Converter


--

Twenty years ago, in 2003, Python 2.3 was released with ``csv.reader()``, a function that provided support for parsing CSV files. The C implementation, proposed in PEP 305, defines a core tokenizer that has been a reference for many subsequent projects. Two commonly needed features, however, were not addressed in ``csv.reader()``: determining type per column, and converting strings to those types (or columns to arrays). Pandas ``read_csv()`` implements automatic type conversion and realization of columns as NumPy arrays (delivered in a DataFrame), with performance good enough to be widely regarded as a benchmark. Pandas implementation, however, does not support all NumPy dtypes. While NumPy offers ``loadtxt()`` and ``genfromtxt()`` for similar purposes, the former (recently re-implemented in C) does not implement automatic type discovery, while the latter (implemented in Python) suffers poor performance at scale.

To support reading delimited files in StaticFrame (a DataFrame library built on an immutable data model), I needed something different: the full configuration options of Python's ``csv.reader()``; optional type discovery for one or more columns; support for all NumPy dtypes; and performance competitive with Pandas ``read_csv()``.

Following the twenty-year tradition of extending ``csv.reader()``, I implemented ``delimited_to_arrays()`` as a C extension to meet these needs. Using a family of C functions and structs, Unicode code points are collected per column (with optional type discovery), converted to C-types, and written into NumPy arrays, all with minimal ``PyObject`` creation or reference counting. Incorporated in StaticFrame, performance tests across a range of DataFrame shapes and type heterogeneity show significant performance advantages over Pandas. Independent of usage in StaticFrame, ``delimited_to_arrays()`` provides a powerful new resource for converting CSV files to NumPy arrays. This presentation will review the background, architecture, and performance characteristics of this new implementation.


The Challenge of Building Columnar Arrays from CSV
-----------------------------------------------------

CSV and related delimited text file formats remain a necessary evil. While numerous high-performance binary alternatives exist (such as Parquet or NPZ), CSV is still popular: it is human readable, widely available, and importable by countless applications and libraries.

Importing such text formats in a DataFrame library raise two challenges. First, the data has to be read row by row, while ultimately, data is organized and typed by column. If attempting to load data in a single iteration, the number of columns or rows cannot be known in advance. Second, no type information is declared within the CSV file; if not explicitly provided to the reader, types must discovered and converted or left as strings.

This implementation extends ``csv.reader()`` to store Unicode code points in a dynamic C-array per column. After the file is read, values in these C-arrays are converted to C-types and directly inserted into NumPy array buffers. The usage of dynamically allocated C-arrays, and a dynamically allocated collection of those arrays, permits loading from file and writing to array in two iterations of the data.


Twenty Years of CSV Parsing in Python
----------------------------------------

Based on PEP 305 in 2003, ``csv.reader()`` was implemented as a C-extension and first included in Python 2.3. Both reader and writer expose a variety of options to support the full diversity of delimited file dialects, including quoting and escaping configurations.

The reader returns an iterator of lists of strings, representing the content parsed from each row. While very efficient for returning Python strings, converting these strings into Python types or columnar NumPy arrays is not addressed.

>>> list(csv.reader(StringIO('a,b\nx,y')))
[['a', 'b'], ['x', 'y']]


The C-implementation of the reader implements a character by character state machine tokenizer. The architecture of this state machine, including some of the same naming, is found in many subsequent implementations, and was reused in the implementation of ``delimited_to_arrays()``.

As CSV files commonly contain columnar-typed data, extending ``csv.reader()`` to build arrays per column is a natural extension. Both Pandas and NumPy have offered resources to meet this need.

Pandas ``read_csv()`` extended ``csv.reader()`` to build array data suitable for returning a DataFrame. An important feature added by Pandas is the ability to automatically discovery a type per column.

Pandas ``read_csv()`` is narrowly implemented for Pandas DataFrames. As such, not all NumPy dtypes are supported. For example, Pandas returns all string data as NumPy object arrays with Python strings, even if a NumPy Unicode array is explicitly requested.

>>> pd.read_csv(StringIO('a,b\nx,y')).dtypes
a    object
b    object
dtype: object
>>> df = pd.read_csv(StringIO('a,b\nx,y'), dtype=np.dtype('U3'))
TypeError: the dtype <U3 is not supported for parsing


Additionally, the full range of NumPy ``datetime64`` units is not supported; if dates are interpreted, they are always interpreted in NumPy ``datetetime64`` nanosecond units, even if the input string is clearly just a date.

>>> pd.read_csv(StringIO('a,b\n2022-01-02,1984-05-22'), dtype=np.datetime64)
TypeError: the dtype datetime64 is not supported for parsing, pass this column using parse_dates instead
>>> pd.read_csv(StringIO('a,b\n2022-01-02,1984-05-22'), parse_dates=[0, 1]).dtypes
a    datetime64[ns]
b    datetime64[ns]


NumPy offers two functions for converting text to arrays: ``loadtxt()`` and ``genfromtxt()``. Unlike ``read_csv()``, both support all NumPy dtypes; the former, however, does not perform automatic type discovery, and the later, implemented in Python, does not sufficiently scale. Additional, neither interface implements the full range of CSV dialects and configurability supported by ``csv.reader()``.


Implementation of ``delimited_to_arrays()``
-----------------------------------------------

The core CSV file parsing and NumPy array creation is implemented in the C-extension function ``delimited_to_arrays()``, part of the ArrayKit Python package. StaticFrame calls ``delimited_to_arrays()`` to convert column data to immutable NumPy arrays, which are then used to build a complete DataFrame.

In the tradition of Pandas, ``delimited_to_arrays`` extends the state machine tokenizer from, and supports the full configuration options of, ``csv.reader()``.

Critical to achieving excellent performance in a Python CSV reader C-extension is avoiding, as much as possible, ``PyObject`` creation and reference counting. This is achieved in ``delimited_to_arrays()`` by using a family of C struct containers and dynamic C arrays of C types to store column data.

The core C-struct containers are the dynamically-allocated CodePointLine (CPL) and CodePointGrid (CPG). The CPL is a struct containing two dynamically-allocated C-arrays: ``buffer``, or an array of Unicode code points (``Py_UCS4``), and ``offsets``, an array of integers (``Py_ssize_t``). Optionally, if type parsing is active, an additional struct is included in the CPL. Code points, read from the delimited file per column, are written to ``buffer``. At the end of each field, the number of code points in that field is written to ``offsets``. This approach permits rapid sequential access to field data without using null-terminators to partition fields.

A CPG stores one or more CPLs, dynamically allocating new CPLs as new columns are observed. Figure 1 illustrates the contents of these containers.

The processing stages of ``delimited_to_arrays()`` are outlined in Figure 2. The input is an iterator of string "records". At initialization, a ``DelimitedReader`` is given that iterator, as well as access to a ``CodePointGrid``. The first iteration stage parses the delimited file, calling the CPG to append code points and offsets to the appropriate CPLs. After record iteration is complete, the second iteration stage converts CPL field data to C-types and writes them in the buffer of a NumPy array. The final output is a list of NumPy arrays.

Figure 3 details the processing of each record in the ``DelimitedReader``. For each code point in the record, a state is determined and the appropriate action taken. A simplified control flow is illustrated: if the code point is not the delimiter, CPG Append Point is called; else, CPG Append Offset is called. The full state machine implementation is far more complicated, handling configurable escaped and quoted values.

Figure 4 details the CPG Append Point process. For the current field number, the CPG is evaluated to determine if a new CPL needs to be dynamically allocated. For the additional code point, the CPL is evaluated to determine if it needs to be resized. Next, if type parsing is activate, the code point is passed to the type parser's Process Char function. The type parsing routine is a simple heuristic approach based on the count of distinguishing characters, determining if a field is a string, float, complex, integer, or Boolean. Finally, the code point is appended to the CPL.

Figure 5 details the CPG Append Offset process. Just as when appending code points, both the CPG and the CPL are evaluated to determine if they need to grow. If type parsing is active, a field type is determined; this field type is evaluated in the context of the line type to resolve a new line type. The offset is appended to the offsets buffer, and the maximum observed offsets is updated if necessary.

The second iteration is bound within the CPG To Array List process. As shown in Figure 6, for each CPL, a dtype is determined. With the dtype and the number of offsets (equivalent to the number of elements in the array), an empty NumPy array of the appropriate dtype and size is created. Using the NumPy C-API's ``PyArray_DATA()`` function, raw C array is accessed from the NumPy array. Iterating over offsets in the CPL, code points are converted to C-types with a family of custom functions, and then those C-types are written into the array data array. This loop happens without any creation or memory management of intermediary PyObjects.

After CPL data is converted and written to the NumPy array, the array is appended to Python list. After all CPLs are processed, the Python list is returned to the user.


Performance
----------------

As illustrated above, the usage of C-types for CPL storage, and the conversion of those values to C-types for insertion in a pre-alliocated NumPy buffer, happens entirely without PyObject creation or reference counting. This is critical to the performance of these routines.

Too often cross-library performance studies use a single table for performance tests. Variation in table shape, as well as variation in columnar type heterogeneity, greatly alter performance, depending on underlying implementation details in DataFrame libraries.

To evaluate the performance of ``delimited_to_arrays()``, and by extension StaticFrame's ``sf.Frame.from_delimited()``, comparison is made to Pandas ``read_csv()`` (using the C-engine) across three different shapes of 100 million (1e8) elements: tall (1e5 x 1e3), square (1e4 x 1e4), and wide (1e3 x 1e5). For each of those three shapes, three types of type heterogeneity are evaluated: columnar (mixtures of floats, ints, strings, and Booleans, with no adjacent types the same), mixed (mixtures of types, with four-column type adjacency), and uniform (all float).

For each DataFrame configuration under test, three approaches to handling type discovery are compared: automatic type discovery (type parsing), no type discovery (simply loading data as strings), and creation from explicitly provided dtypes.

StaticFrame out-performs Pandas for all square and wide formations, for all approaches to type discovery. For tall formations, StaticFrame type discovery is shown to be slightly slower than Pandas, but for all other type discovery approaches, StaticFrame is shown to be faster.

Pandas is shown to be significantly slower in all cases where all data is returned as strings. This is likely due to Pandas returning object arrays of Python string objects, whereas StaticFrame returns NumPy Unicode arrays. The creation of Python objects creates creates significant overhead compared to just writing Unicode code points directly into a Unicode array.


Future Enhancements
-----------------------

In the current implementation of ``delimited_to_arrays``, the second stage of iteration is done sequentially in one thread. As there is a CPL for each column, there might be thousands or hundreds of thousands of columns: converting them to NumPy arrays is a embarrassingly parallel problem. Future work to make this second iteration parallel will further improve performance.

Additionally, the ``buffer`` used in each CPL is four-byte Py_UCS4, regardless if the underlying unicode might fit in smaller Py_UCS2 or Py_UCS1. NumPy's new implementation of ``np.loadtxt`` uses C++ template to explicitly support all three sizes of Unicide code points. The ``delimited_to_arrays`` routine might adopt a similar approach, potentially reducing memory overhead during loading.


Conclusion
-----------------------

The ArrayKit ``delimited_to_arrays()`` function offers high-performance CSV-to-array conversion, with the same level of dialect support configurability as found in Python's ``csv.reader()``. Employed in StaticFrame, delimited files can be realized as DataFrames significantly faster than Pandas default C-engine, and opportunities for further performance enhancements remain.




==
Not used:


One of the PEPs authors, Dave Cole, had since 2000 released a stand-alone C-extension CSV parser. The current Python implementation still bares similarity to the Cole implementation from over 20 years ago.

There have been many attempts to address this challenge. For nearly twenty years, the ``csv.reader`` within the Python standard library has provided high-performance parsing of delimited files, but does not infer or convert types, let alone build NumPy arrays. NumPy's built-in ``genfromtxt()`` and ``loadtxt()`` do not support the full diversity of CSV files commonly encountered. While the Pandas CSV reader offers good performance, it does not support all NumPy array dtypes.

With over 50 parameters, ``read_csv()`` is highly configurable, though some parameters, such as ``keep_default_na`` and ``na_values`` and ``na_filter`` are confusingly interrelated, and other parameters, such as ``storage_options`` are only there to support network based input file retrieval.


The ``loadtxt()`` function, while recently reimplemented in C, does not support headers and requires types per column to be specified as structured arrays. It is designed more as a loader of output from the corresponding ``savetxt()`` than general purpose CSV reader. There is not support for automatic type discovery.

>>> np.loadtxt((StringIO('a,b\nx,1.2\ny,nan')), dtype=[('a',str), ('b',float)], delimiter=',', skiprows=1, unpack=True)
[array(['', ''], dtype='<U0'), array([1.2, nan])]

While the ``genfromtxt()`` function offers broad flexibility for a range of delimited files and does support automatic type discovery, it is written in pure Python: its performance scales poorly with large CSV files.

>>> np.genfromtxt((StringIO('a,b\nx,1.2\ny,nan')), dtype=None, delimiter=',', skip_header=1)
array([(b'x', 1.2), (b'y', nan)], dtype=[('f0', 'S1'), ('f1', '<f8')])


Then, in the second iteration, when creating new NumPy arrays, values from those C arrays are directly written to the NumPy array's underlying C-buffer (accessed via ``PyArray_DATA``).

NumPy loadtxt C++ impl: https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/textreading/tokenize.cpp

Conversion routines: https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/textreading/conversions.c



On arrow string encoding
https://pola-rs.github.io/polars/polars/docs/performance/index.html



==

Notes:

I am aware that presenting on the architecture of a large Python C-extension is difficult. I will support my presentation by using a number of diagrams and flowcharts. Little, if any, C code will be displayed.


Outline:

Twenty Years of CSV Parsing in Python (4 min)
    Python 2.3's ``csv.reader()`` in 2003
        C implementation of state machine tokenizer
        Configuration for a wide range of CSV dialects
        Returns lists of strings per rows
    The Pandas ``read_csv()``
        Featured automatic type discovery per column
        No support for NumPy Unicode arrays
        No support for NumPy datetime64 units other than nanonsecond
        Widely regarded as a performance benchmark
    NumPy's ``np.loadtxt()``
        No automatic type discovery
        Missing full support of ``csv.reader()`` dialects
    NumPy's ``np.genfromtxt()``
        Insufficient performance at scale
        Missing full support of ``csv.reader()`` dialects

Goals for a New Implementation (2 min)
    Support the full configuration of ``csv.reader()`` dialects
    Permit optionally discovering types per column
    Support all NumPy dtypes
    Performance competitive to Pandas ``read_csv()``

The Challenge of Building Columnar Arrays from CSV (2 min)
    CSV is a necessary evil
        Human readable
        Still widely used
        Importable by many applications and libraries
        Always slower then binary formats
    Cannot know number of columns or rows in advance
    Type per column is not encoded in file
    Two full iterations are required
        Dynamic C arrays per column collect Unicode code points, optionally discovering type
        Values per field are converted to C-types and written  into NumPy array buffers

Implementation of ``delimited_to_arrays()`` (10 min)
    C-extension implementation as part of ArrayKit
    Re-use of ``csv.reader()`` tokenizer
    Re-use of all ``csv.reader()`` dialect configuration options
    Core C-struct Containers (Figure 1)
        CodePointLine (CPL)
            Dynamic array of Py_UCS4: buffer
            Dynamic array of Py_ssize_t: offsets
            Optional struct to store type discovery metrics
        CodePointGrid (CPG)
            Public interface to CPLs
            Dynamically create and populate CPLs
    Processing Stages (Figure 2)
        Input: Iterator of string records
        Initialization
            DelimitedReader
            CodePointGrid
        First iteration
            Iterate records, tokenize fields
            Call CPG Append Point
            Call CPG Append Offset
        Second iteration
            Iterate CPLs, convert values, write to arrays
            Call CPG To Array List
        Output: A list of NumPy arrays
    DelimitedReader processing (Figure 3)
        If code-point is not delimiter, call CPG Append Point
        If code-point is delimiter, call CPG Append Offset
    CPG Append Point (Figure 4)
    CPG Append Offset (Figure 5)
    CPG To Array List (Figure 6)

Performance (6 min)
    The importance of avoiding PyObject creation and reference counting
        Numerous processes do not involve PyObjects
            Tokenization and CPL creation
            Appending to CPLs, type discovery
            CPL value conversion
            Writing to NumPy array buffers
        Critical to performance
    Performance test cases
        Test nine table compositions with three type-discovery methods for both StaticFrame and Pandas
        Variation in table shape of 100 million elements
            Wide (1e3 x 1e5)
            Square (1e4 x 1e4)
            Tall (1e5 x 1e3)
        Variation in column type heterogeneity
            Columnar: mixtures of types, no same-type adjacency
            Mixed: mixtures of types, four-column same-type adjacency
            Uniform: all float
        Variation in type discovery method
            Automatic type discovery
            No type discovery (returning strings)
            Array creation from user-provided types
    Results by shape
        Wide cases
            StaticFrame out-performs Pandas for all variations
        Square cases
            StaticFrame out-performs Pandas for all variations
        Tall cases
            Pandas type discovery shown to be faster
            StaticFrame out-performs all other variations
        Pandas string conversion shown to be a detriment
            StaticFrame uses NumPy unicode arrays
            Pandas uses object arrays of Python string objects

Future Enhancements (4 min)
    Parallelization of CPL array creation and loading
        Converting CPLs to arrays an embarrassingly parallel problem
    Exclusive use of ``Py_UCS4`` in CPL buffers
        Often only ``Py_UCS2`` or ``Py_UCS1`` is needed
        Usage will reduce memory overhead, increase performance

Conclusion (2 min)
    ``delimited_to_arrays()`` offers high-performance creation of arrays from CSV files
        Full support of ``csv.reader()`` dialect configuration
        Optional automatic type discovery per column
        Support for all NumPy array dtypes
        Significant performance advantages over Pandas ``read_csv()``.
    Additional opportunities for performance enhancements remain



Past Experience

    Please summarize your teaching or public speaking experience, as well as your experience with the subject. Provide links to one (or two!) previous presentations by each speaker. If you have any additional notes, they can be placed here as well.


I have presented at numerous national and international conferences in many domains over the last twenty years, and taught as a university professor of music technology for six years, frequently teaching technical topics. I have presented at numerous PyData conferences and at one PyCon.

I have been programming in Python since the year 2000, I have been writing production Python for financial systems for over ten years, and I am expert in NumPy, Pandas, StaticFrame, and DataFrame libraries in general.

Examples of recent presentations:

PyCon US 2022: "Employing NumPy's NPY Format for Faster-Than-Parquet DataFrame Serialization": https://youtu.be/HLH5AwF-jx4

PyData Global 2021: "Why Datetimes Need Units: Avoiding a Y2262 Problem & Harnessing the Power of NumPy's datetime64": https://www.youtube.com/watch?v=jdnr7sgxCQI




























